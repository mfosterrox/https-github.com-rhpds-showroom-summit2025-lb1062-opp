= Policy-Driven Governance with RHACM

= Setup

[[access]]

== Environment Setup and Access

[[openshift-acm]]

===  OpenShift console access & RHACM console access

NOTE: The Red Hat Advanced Cluster Management for Kubernetes (RHACM) Console is now integrated with OpenShift Console, to access it please select *All Clusters* from the dropdown menu once you log in. 

Your OpenShift cluster console is available at: {openshift_console_url}

Administrator login is available with:

[cols="1,1"]
|===
| *Username:* | kubeadmin
| *Password:* | {openshift_kubeadmin_password}
|===

[[acs]]
=== RHACS console access

Your RHACS Console is available at: {acs_route}[window=blank]

Administrator login is available with:

[cols="1,1"]
|===
| *RHACS Console Username:* | {acs_portal_username} 
| *RHACS Console Password:* | {acs_portal_password} 
|===

=== roxctl CLI verification 

Next, verify that you have access to the RHACS Central Service.

*Procedure*

[start=1]
. Run the following command.

====
This command uses variables saved in the ~/.bashrc file to authenticate with the RHACS Central Service.
====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
UserID:
	auth-token:718744a9-9548-488b-a8b9-07b2c59ea5e6
User name:
	anonymous bearer token "pipelines-ci-token" with roles [Admin] (jti: 718744a9-9548-488b-a8b9-07b2c59ea5e6, expires: 2025-04-03T15:15:06Z)
Roles:
	- Admin
Access:
	rw Access
	rw Administration
	rw Alert
	rw CVE
	rw Cluster
	rw Compliance
	rw Deployment
	rw DeploymentExtension
	rw Detection
	rw Image
	rw Integration
	rw K8sRole
	rw K8sRoleBinding
	rw K8sSubject
	rw Namespace
	rw NetworkGraph
	rw NetworkPolicy
	rw Node
	rw Secret
	rw ServiceAccount
	rw VulnerabilityManagementApprovals
	rw VulnerabilityManagementRequests
	rw WatchedImage
	rw WorkflowAdministration
----

NOTE: This output indicates you have full access to the RHACS product. You can check these permissions in the RHACS Access Control tab, which you'll review later.

If you have full access, you're all set!

== Deploy the workshop applications

You now have access to the core OPP applications. Next, you'll deploy several insecure apps into the OpenShift cluster. These demo apps come from public GitHub repositories and cover various vulnerabilities, including well-known Capture the Flag (CTF) apps, Log4Shell, and Apache Struts.

After deployment, you'll use the roxctl CLI to scan some containers, giving you a preview of what to expect during the security section with Red Hat Advanced Cluster Security.

=== Time to deploy

*Procedure*

[start=1]
. Run the following commands in the terminal, one after the other.

====
This command downloads a repository container dockerfiles, attack scripts, and Kubernetes manifests that you will use to deploy the containerized applications to OpenShift.  
====

[source,sh,subs="attributes",role=execute]
----
git clone https://github.com/mfosterrox/demo-apps.git demo-apps
git clone https://github.com/mfosterrox/skupper-security-demo.git skupper-app
----

====
This command sets the variable TUTORIAL_HOME to equal the working directory, allowing you to make references to various files easily.
====

[source,sh,subs="attributes",role=execute]
----
echo export TUTORIAL_HOME="$(pwd)/demo-apps" >> ~/.bashrc
export TUTORIAL_HOME="$(pwd)/demo-apps"
echo export APP_HOME="$(pwd)/skupper-app" >> ~/.bashrc
export APP_HOME="$(pwd)/skupper-app"
----

====
This command applies the manifests into the OpenShift environment.
====

[source,sh,subs="attributes",role=execute]
----
oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
oc apply -f $APP_HOME/skupper-demo/ --recursive
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
namespace/operations created
namespace/backend created
....
route.route.openshift.io/webgoat created
configmap/webgoat-config created
----

NOTE: You may see warnings like: Warning: would violate PodSecurity "baseline:latest": privileged (container "proxy" must not set securityContext.privileged=true). These warnings appear because you are deploying containers with flawed configurations and known vulnerabilities into the OpenShift cluster.

====
The following command will display what applciations have been deployed to the cluster
====

[source,bash,role="execute"]
----
oc get deployments -l demo=roadshow -A
oc get deployments -n patient-portal
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ oc get deployments -l demo=roadshow -A -w
NAMESPACE    NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
backend      api-server            1/1     1            1           18s
default      dvma                  1/1     1            1           76s
default      juice-shop            1/1     1            1           73s
default      log4shell             1/1     1            1           70s
default      open-api-server       1/1     1            1           39s
default      reporting             1/1     1            1           42s
default      vulnerable-node-app   1/1     1            1           36s
default      webgoat               1/1     1            1           33s
frontend     asset-cache           1/1     1            1           66s
medical      reporting             1/1     1            1           58s
operations   jump-host             1/1     1            1           54s
payments     visa-processor        1/1     1            1           52s
----

IMPORTANT: Please ensure the deploy application are deployed and available before moving onto the next module. 

====
The following command triggers a vulnerability scan by RHACS, roxctl filters the results into a table. The severity flag means only the critical vulnerabilities will be shown. This image is known as the "Damn Vulnerable Wed Application" and it contains A LOT of vulnerabilities.
====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=quay.io/mfoster/dvwa --force -o table
----

TIP: The following output can be configured using flags. You can configure different outputs (table, CSV, JSON, and sarif.) and filter for specific severities.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=quay.io/mfoster/dvwa --force -o table 
Scan results for image: quay.io/mfoster/dvwa

---------------------------------+------------------+
|          zlib1g           |     1:1.2.13.dfsg-1     |   CVE-2023-45853    | CRITICAL  |         https://nvd.nist.gov/vuln/detail/CVE-2023-45853         |       -       |
+---------------------------+                         +---------------------+-----------+-----------------------------------------------------------------+---------------+
|        zlib1g-dev         |                         |   CVE-2023-45853    | CRITICAL  |         https://nvd.nist.gov/vuln/detail/CVE-2023-45853         |       -       |
+---------------------------+-------------------------+---------------------+-----------+-----------------------------------------------------------------+---------------+
WARN:   A total of 1174 unique vulnerabilities were found in 106 components
----

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

== Implement Policy-as-Code in ACM with OpenShift GitOps

Here, you will manage ACS security policies via RHACM and OpenShift GitOps. First, you must create an ArgoCD controller in RHACM.

===  RHACM console access

The RHACM console is available in the OpenShift cluster console at: {openshift_console_url}

Administrator login is available with:

[cols="1,1"]
|===
| *Username:* | kubeadmin 
| *Password:* | {openshift_kubeadmin_password}
|===

Navigate to the *Cluster* drop down menu and then select *All Clusters*. 

image::101-local-cluster.png[link=self, window=blank, width=100%, Cluster Selection Menu]

In this interface you will see 2 clusters available, the first cluster is a Hosted Control Plane Cluster called *"development"*. The second cluster is our working cluster labeled *"local-cluster"*.

image::102-cluster-view.png[link=self, window=blank, width=100%, View of Clusters Listed]

[[create-manage-cluster]]

=== Integrate ArgoCD with RHACM

*Procedure*

. Navigate to *Applications* from the left side menu.
. Click *Create application, select ArgoCD AppicationSet-Push Model*.
. Under the Argo server select *Add Argo Server* 
. Enter the following information:
* *Name:* openshift-gitops
* *Namespace:* openshift-gitops
* *ClusterSet:* default

image::03-argoconfig.png[link=self, window=blank, width=100%, ArgoCD Config]

=== Implement security policy-as-code

Next, you will use that argo server to deploy security policies for RHACS. First, let's check on the policies currently available.

*Procedure*

[start=1]
. Perform an image security scan using roxctl to check for policy violations.

[source,sh,subs="attributes",role=execute]
----
roxctl -e $ROX_CENTRAL_ADDRESS:443 image check --image quay.io/mfoster/frontend:latest
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Policy check results for image: quay.io/mfoster/frontend:latest
(TOTAL: 3, LOW: 3, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|             POLICY             | SEVERITY | BREAKS BUILD |          DESCRIPTION           |           VIOLATION            |          REMEDIATION           |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|        90-Day Image Age        |   LOW    |      -       |   Alert on deployments with    |     - Image was created at     |   Rebuild your image, push a   |
|                                |          |              |    images that haven't been    |   2025-01-18 21:14:40 (UTC)    | new minor version (with a new  |
|                                |          |              |       updated in 90 days       |                                |   immutable tag), and update   |
|                                |          |              |                                |                                |    your service to use it.     |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|  Alpine Linux Package Manager  |   LOW    |      -       | Alert on deployments with the  |   - Image includes component   |      Run `apk --purge del      |
|         (apk) in Image         |          |              |  Alpine Linux package manager  |      'apk-tools' (version      | apk-tools` in the image build  |
|                                |          |              |         (apk) present          |           2.14.6-r2)           |   for production containers.   |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|           Latest tag           |   LOW    |      -       |   Alert on deployments with    |    - Image has tag 'latest'    |  Consider moving to semantic   |
|                                |          |              |   images using tag 'latest'    |                                |    versioning based on code    |
|                                |          |              |                                |                                | releases (semver.org) or using |
|                                |          |              |                                |                                | the first 12 characters of the |
|                                |          |              |                                |                                | source control SHA. This will  |
|                                |          |              |                                |                                |  allow you to tie the Docker   |
|                                |          |              |                                |                                |       image to the code.       |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
WARN:   A total of 3 policies have been violated
----

> 3 policies have been violated but no enforcement is available. Let's change that.

==== Implement then policies

link:https://github.com/mfosterrox/skupper-security-demo/tree/main/PaC-custom-policies[PaC Custom Policies on GitHub]

*Procedure*

. First, navigate to the RHACM dashboard and *select the Application tab*

image::04-pac-01.png[link=self, window=blank, width=100%]

[start=2]
. Create a new application -> In the dashboard, click on "Create Application" to create a new application & select *"Push model".*
. Configure the Application:
* Name: Give the application a name, e.g., pac-custom-policies.
* Select the Argo server, "openshift-gitops"

image::04-pac-02.png[link=self, window=blank, width=100%]

[start=4]
. Click "*Next*"
. In the Template tab, select *Git Repository* and enter the URL of the GitHub repository containing the custom policies - URL: https://github.com/mfosterrox/skupper-security-demo.git
. Select "Main"
. Select "PaC-custom-policies"
. Enter the remote namespace of "stackrox"

image::04-pac-03.png[link=self, window=blank, width=100%]

[start=9]
. Click "*Next*"
. in the "Sync policy" tab select 
* *Replace resources instead of applying changes from the source repository*

image::04-pac-04.png[link=self, window=blank, width=100%]

[start=11]
. Click "*Next*"
. Set the following in the Placement tab
* Cluster sets: *default*
. Under *Label expressions* click *add label* and select the following
* *Label:* name
* *Operator:* equals any of
* *Values:* local-cluster

image::04-pac-05.png[link=self, window=blank, width=100%]

[start=14]
. Click "*Next*"
. Click "*Submit*"

== Ensure that the policies are properly configured

Let's run the check again to check if the policies were successful.

*Procedure*

[start=1]
. Perform an image security scan using roxctl to check for policy violations.

[source,sh,subs="attributes",role=execute]
----
roxctl -e $ROX_CENTRAL_ADDRESS:443 image check --image quay.io/mfoster/frontend:latest
----


[.console-output]
[source,bash,subs="+macros,+attributes"]
----
(TOTAL: 4, LOW: 4, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|             POLICY             | SEVERITY | BREAKS BUILD |          DESCRIPTION           |           VIOLATION            |          REMEDIATION           |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|        90-Day Image Age        |   LOW    |      -       |   Alert on deployments with    |     - Image was created at     |   Rebuild your image, push a   |
|                                |          |              |    images that haven't been    |   2025-01-18 21:14:40 (UTC)    | new minor version (with a new  |
|                                |          |              |       updated in 90 days       |                                |   immutable tag), and update   |
|                                |          |              |                                |                                |    your service to use it.     |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|  Alpine Linux Package Manager  |   LOW    |      -       | Alert on deployments with the  |   - Image includes component   |      Run `apk --purge del      |
|         (apk) in Image         |          |              |  Alpine Linux package manager  |      'apk-tools' (version      | apk-tools` in the image build  |
|                                |          |              |         (apk) present          |           2.14.6-r2)           |   for production containers.   |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|  Alpine Linux Package Manager  |   LOW    |      X       | Alert on deployments with the  |   - Image includes component   |      Run `apk --purge del      |
|   (apk) in Image - Build and   |          |              |  Alpine Linux package manager  |      'apk-tools' (version      | apk-tools` in the image build  |
|        Deploy - Enforce        |          |              |         (apk) present          |           2.14.6-r2)           |   for production containers.   |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
|           Latest tag           |   LOW    |      -       |   Alert on deployments with    |    - Image has tag 'latest'    |  Consider moving to semantic   |
|                                |          |              |   images using tag 'latest'    |                                |    versioning based on code    |
|                                |          |              |                                |                                | releases (semver.org) or using |
|                                |          |              |                                |                                | the first 12 characters of the |
|                                |          |              |                                |                                | source control SHA. This will  |
|                                |          |              |                                |                                |  allow you to tie the Docker   |
|                                |          |              |                                |                                |       image to the code.       |
+--------------------------------+----------+--------------+--------------------------------+--------------------------------+--------------------------------+
WARN:   A total of 4 policies have been violated
ERROR:  failed policies found: 1 policies violated that are failing the check
ERROR:  Policy "Alpine Linux Package Manager (apk) in Image - Build and Deploy - Enforce" - Possible remediation: "Run `apk --purge del apk-tools` in the image build for production containers."
ERROR:  checking image failed: failed policies found: 1 policies violated that are failing the check
----

> You should see the change in the output to show the new policy violation. RHACS will block this deployment if you try to deploy it again. 

== Creating Policies in RHACM

In order to assist in the creation and management of Red Hat Advanced Cluster Management for Kubernetes policies you use the policy generator tool. This tool, along with GitOps, greatly simplifies the distribution of Kubernetes resource objects to managed OpenShift or Kubernetes clusters through RHACM policies.

=== Prerequisites

To deploy policies with subscriptions, you will need to bind the *open-cluster-management:subscription-admin* ClusterRole to the user creating the subscription.

Procedure

. Navigate to the *Governance* tab.
. On the top tabs, click on *Policies*.
. Click *Create Policy*.
. On the top switch the toggle to *Display the YAML*.

image::114-policy-toggle.png[link=self, window=blank, width=100%, Display the YAML]

[start=5]
. Copy the following YAML excerpt and paste it in the screen:

[source,yaml,role=execute]
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-configure-subscription-admin-hub
  namespace: ""
  annotations:
    policy.open-cluster-management.io/standards: NIST SP 800-53
    policy.open-cluster-management.io/categories: CM Configuration Management
    policy.open-cluster-management.io/controls: CM-2 Baseline Configuration
spec:
  remediationAction: inform
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-configure-subscription-admin-hub
        spec:
          remediationAction: inform
          severity: low
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: rbac.authorization.k8s.io/v1
                kind: ClusterRole
                metadata:
                  name: open-cluster-management:subscription-admin
                rules:
                  - apiGroups:
                      - app.k8s.io
                    resources:
                      - applications
                    verbs:
                      - "*"
                  - apiGroups:
                      - apps.open-cluster-management.io
                    resources:
                      - "*"
                    verbs:
                      - "*"
                  - apiGroups:
                      - ""
                    resources:
                      - configmaps
                      - secrets
                      - namespaces
                    verbs:
                      - "*"
            - complianceType: musthave
              objectDefinition:
                apiVersion: rbac.authorization.k8s.io/v1
                kind: ClusterRoleBinding
                metadata:
                  name: open-cluster-management:subscription-admin
                roleRef:
                  name: open-cluster-management:subscription-admin
                  apiGroup: rbac.authorization.k8s.io
                  kind: ClusterRole
                subjects:
                  - name: kube:admin
                    apiGroup: rbac.authorization.k8s.io
                    kind: User
                  - name: system:admin
                    apiGroup: rbac.authorization.k8s.io
                    kind: User
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: policy-configure-subscription-admin-hub-placement
  namespace: ""
placementRef:
  name: policy-configure-subscription-admin-hub-placement
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
  - name: policy-configure-subscription-admin-hub
    kind: Policy
    apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: policy-configure-subscription-admin-hub-placement
  namespace: ""
spec:
  clusterConditions:
    - status: "True"
      type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      - key: name
        operator: In
        values:
          - local-cluster
----


[start=6]
. Enter a namespace to place the policy, the *default* namespace is OK to use.
. Click *Next* till the end and then click on *Submit*.
. Allow a few moments for the policy to propagate to the *local-cluster / RHACM Hub Cluster*.
. Navigate back to policies and select the *policy-configure-subscription-admin-hub* policy.
. In the "Policy templates" select *Enforce*. This will enforce the policy, wait until the green checkmark is displayed.

image::115-enforce-governance-policy.png[link=self, window=blank, width=100%, Enforce the Governance Policy]

=== Using Policy Generator

This Policy Generator description will create 2 configuration policies:

* *openshift-gitops-installed*: The goal of the first policy is to inform if the OpenShift GitOps operator is installed on managed clusters.
* *kubeadmin-removed*: The goal of the second policy is to inform if the kubeadmin user is removed from managed clusters.

NOTE: Both policies are informative only, and you will only execute them manually to demonstrate how to resolve issues.

With the ArgoCD resource, you can use Red Hat OpenShift GitOps to manage policy definitions by granting OpenShift GitOps access to create policies on the Red Hat Advanced Cluster Management hub cluster.

==== Prerequisites

We need to create a *ClusterRole* resource for OpenShift GitOps, with access to create, read, update, and delete policies and placements: 

*Procedure*

. Create a *ClusterRole* from the OCP Console by Clicking the + Icon on the Top right, then select "Import YAML".

image::04-ocp-console-add.png[link=self, window=blank, width=100%, OCP Console Add]

[start=2]
. Copy and Paste the following YAML, Click *Create*

[source,sh,subs="attributes",role=execute]
----
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openshift-gitops-policy-admin
rules:
  - verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
    apiGroups:
      - policy.open-cluster-management.io
    resources:
      - policies
      - configurationpolicies
      - certificatepolicies
      - operatorpolicies
      - policysets
      - placementbindings
  - verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
    apiGroups:
      - apps.open-cluster-management.io
    resources:
      - placementrules
  - verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
    apiGroups:
      - cluster.open-cluster-management.io
    resources:
      - placements
      - placements/status
      - placementdecisions
      - placementdecisions/status
----

[start=3]
. Navigate back to the + sign again, you need the *ClusterRoleBinding* object to grant the OpenShift GitOps service account access to the *openshift-gitops-policy-admin* ClusterRole object. Click *Create* once complete

[source,sh,subs="attributes",role=execute]
----
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openshift-gitops-policy-admin
subjects:
  - kind: ServiceAccount
    name: openshift-gitops-argocd-application-controller
    namespace: openshift-gitops
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: openshift-gitops-policy-admin
----

[start=4]
. Navigate to the Terminal in the Showroom tab and execute the following command

[source,sh,subs="attributes",role=execute]
----
oc -n openshift-gitops patch argocd openshift-gitops --type merge --patch "$(curl https://raw.githubusercontent.com/jalvarez-rh/grc-policy-generator-blog/refs/heads/main/openshift-gitops/argocd-patch.yaml)"
----

----
This command will merge and patch the ArgoCD configuration to point to the latest Init Container that will allow us to deploy the Policies 
----

=== Deploying a Policy with ArgoCD

*Procedure*

[start=1]
. Navigate to *Applications*.
. Click *Create application, select ArgoCD ApplicationSet - Push Model*.
* Name: policy-generator
* Argo server: openshift-gitops
* Click *NEXT*
* Under repository types, select the GIT repository
* URL: https://github.com/jalvarez-rh/demo-policygenerator.git
* Revision: main
* Path: dir
* Remote namespace: openshift-gitops
* Click *NEXT* Twice

. Verify that it installs only to the local cluster by setting the following values:
* Cluster sets: *default*
* Label: *local-cluster*
* Operator: *equals any of*
* Value: *true*

. Verify all the information is correct, click *Create*.

It will take a few minutes to deploy the application, *Click on the Topology Tab* to view and verify that *all of the circles are green*.

image::116-governance-topology.png[link=self, window=blank, width=100%, Governance Topology]

[start=5]
. Navigate to the *Governance* tab.
. Click on the *Policies* tab.
. Verify that you see two policies and that their *Cluster Violations* count is one.
* openshift-gitops-installed
* kubeadmin-removed

image::117-policies-list.png[link=self, window=blank, width=100%, Governance Policies List]

Now that the policies have been created for us leveraging the Policy Generator Engine let’s go ahead and enforce them:

[start=8]
. On the *openshift-gitops-installed* policy, click on the ellipses and set policy to *Enforce*.

image::118-policies-enforce-red.png[link=self, window=blank, width=100%, Enforce the Policy]

[start=9]
. Click the *Enforce* button to verify.
. Wait a few minutes and you will see that the *Cluster Violations* will go from *red* to *green*.

image::119-policies-enforce-green.png[link=self, window=blank, width=100%, Policy Enforced]

[start=11]
. Click on the policy and select *Results* to verify that the gitops operator has been installed.

CAUTION: CAREFUL implementing the *kubeadmin-removed* policy, if you enforce this you won’t be able to continue this lab and access that cluster through the console as the only account created on these clusters is Kubeadmin.

== Working with Kyverno Policies in RHACM

Kyverno is a Kubernetes-native policy engine that allows you to manage configurations and enforce governance across your Kubernetes clusters using YAML. It enables you to define policies for security, compliance, and best practices without needing to write custom code.

When integrated with RHACM, Kyverno enhances centralized governance and policy management across multiple clusters. RHACM can distribute Kyverno policies to managed clusters, enabling:
	•	*Consistent security and compliance:* Enforce security standards (e.g., disallow privileged containers) across clusters.
	•	*Automated policy enforcement:* Automatically mutate, validate, or generate resources to meet operational standards.
	•	Multi-cluster visibility: Use RHACM’s observability and policy dashboards to monitor policy compliance at scale.

=== Prerequisites

To deploy Kyverno policies with ArgoCD, you will need to deploy the Kyverno engine in the local hub cluster.

*Procedure*

. Execute the following install script.

[source,sh,subs="attributes",role=execute]
----
kubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.13.0/install.yaml
----

NOTE: Script might fail to create the *policyreports.wgpolicyk8s.io* this is expected behavior, you can move on.

image::05-kyverno-error-install.png[link=self, window=blank, width=100%, Kyverno Install Error]

== Deploying Kyverno Policies in RHACM

*Procedure*

[start=1]
. Navigate to *Applications*.
. Click *Create application, select ArgoCD ApplicationSet - Push Model*.
* Name: kyverno-policy
* Argo server: openshift-gitops
* Click *NEXT*
* Under repository types, select the GIT repository
* URL: https://github.com/jalvarez-rh/rhacm-tmm-kyverno-example.git
* Revision: main
* Path: files
* Remote namespace: openshift-gitops
* Click *NEXT* Twice

. Verify that it installs only to the local cluster by setting the following values:
* Cluster sets: *default*
* Label: *local-cluster*
* Operator: *equals any of*
* Value: *true*

. Verify all the information is correct, click *Create*.

It will take a few minutes to deploy the application, *Click on the Topology Tab* to view and verify that *all of the circles are green*.

image::05-kyverno-policy.png[link=self, window=blank, width=100%, Kyverno Policy Install]

[start=5]
. Navigate to the *Governance* tab.
. Click on the *Discovered Policies* tab.
. Verify that you see a Kyverno policie called *require-labes* and that their *Cluster Violations* count is one.

image::05-kyverno-policy-acm.png[link=self, window=blank, width=100%, Governance Policies List]

Now that the policies have been created for us leveraging the Policy Generator Engine let’s go ahead and have a look at it.

Now you have successfully created a Policy leveraging the Policy Generator to scan your clusters, if you would like to play with other policies please visit the Policy Repo for more Policies you can test out.